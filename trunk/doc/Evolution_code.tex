\documentclass[10pt,a4paper]{article}
\usepackage{amsmath,amssymb,bm,makeidx,subfigure}
\usepackage[italian,english]{babel}
\usepackage[center,small]{caption}[2007/01/07]
\usepackage{fancyhdr}
\usepackage{color}

\begin{document}

\newpage

\section{QCD Evolution Implementation}

The QCD DGLAP equation looks like this:
\begin{equation}\label{dglap}
\mu^{2}\frac{\partial q_{i}(x,\mu^{2})}{\partial \mu^{2}}=\int^{1}_{x}\frac{dy}y P_{ij}\left(\frac{x}{y},\alpha_{s}(\mu^{2})\right)q_{j}(y,\mu^{2})
\end{equation}
Now let us make the following definitions $t=\ln(\mu^{2})$, $\tilde{q}(x,t)=xq(x,\mu^{2})$, $\tilde{P}_{ij}(x,t)=xP_{ij}(x,\alpha_{s}(\mu^{2}))$ so that eq (\ref{dglap}) becomes:
\begin{equation}\label{dglap2}
\frac{\partial \tilde{q}_{i}(x,t)}{\partial t}=\int^{1}_{x}\frac{dy}y \tilde{P}_{ij}\left(\frac{x}{y},t\right)\tilde{q}_{j}(y,t)
\end{equation}

\section{Interpolation}
In order to numerically solve the above equation, we need to write PDFs as interpolated functions over an $x$ grid. In particular we want to have something like this:
\begin{equation}
\tilde{q}(y,t)=\sum^{N_{x}}_{\alpha=0}w_{\alpha}^{(k)}(y)\tilde{q}(x_{\alpha},t)\,,
\end{equation}
where $w_{\alpha}^{(k)}$ are the interpolation functions of degree $k$ we are looking for.

Using the Lagrange formula, we find that:
\begin{equation}\label{LagrangeFormula}
w_{\alpha}^{(k)}(x) = \sum_{j=0,j \leq \alpha}^{k}\theta(x-x_{\alpha-j})\theta(x_{\alpha-j+1}-x)\prod^{k}_{\delta=0,\delta\ne j}\left[\frac{x-x_{\alpha-j+\delta}}{x_{\alpha}-x_{\alpha-j+\delta}}\right]\,.
\end{equation}
This automatically means that:
\begin{equation}\label{nonzero}
w_{\alpha}^{(k)}(x) \neq 0 \quad\mbox{for}\quad x_{\alpha-k} < x < x_{\alpha+1}.  
\end{equation}

This way we have that eq. (\ref{dglap2}) becomes:
\begin{equation}\label{dglap3}
\frac{\partial \tilde{q}_{i}(x,t)}{\partial t}=\sum_{\alpha}\int^{1}_{x}\frac{dy}y \tilde{P}_{ij}\left(\frac{x}{y},t\right)w_{\alpha}^{(k)}(y)\tilde{q}_{j}(x_{\alpha},t)
\end{equation}
more in particular, if $x$ is one of the grid points $x_\beta$, simplifying a bit the notation we have that:
\begin{equation}\label{dglap4}
\frac{\partial \tilde{q}_{i}(x_\beta,t)}{\partial t}=\sum_{\alpha}\underbrace{\left[\int^{1}_{x_\beta}\frac{dy}y \tilde{P}_{ij}\left(\frac{x_\beta}{y},t\right)w_{\alpha}^{(k)}(y)\right]}_{\Pi_{ij,\beta\alpha}(t)}\tilde{q}_{j}(x_{\alpha},t)\,.
\end{equation}
Given eq.~(\ref{nonzero}), it follows the condition:
\begin{equation}\label{nonzero2}
\Pi_{ij,\beta\alpha}(t) \neq 0 \quad\mbox{for}\quad \beta \leq \alpha.  
\end{equation}
In addition, the integral in eq.~(\ref{dglap4}) which gives $\Pi_{ij,\beta\alpha}$ can be optimized again using eq.~(\ref{nonzero}) and it can be written as:
\begin{equation}\label{optimization}
\Pi_{ij,\beta\alpha}(t) = \int^{b}_{a}\frac{dy}y \tilde{P}_{ij}\left(\frac{x_\beta}{y},t\right)w_{\alpha}^{(k)}(y)
\end{equation}
where:
\begin{equation}
a =  \mbox{max}(x_\beta,x_{\alpha-k})\quad\mbox{and}\quad b = \mbox{min}(1,x_{\alpha+1})\,.
\end{equation}
However, performing a change of variables, $\Pi_{ij,\beta\alpha}$ can also be written as:
\begin{equation}\label{optimization2}
\Pi_{ij,\beta\alpha}(t) = \int^{d}_{c}\frac{dy}y \tilde{P}_{ij}(y,t)w_{\alpha}\left(\frac{x_\beta}{y}\right)
\end{equation}
where this time:
\begin{equation}\label{bounds2}
c =  \mbox{max}(x_\beta,x_\beta/x_{\alpha+1}) \quad\mbox{and}\quad d = \mbox{min}(1,x_\beta/x_{\alpha-k}) \,.
\end{equation}
Verifying that eqs.~(\ref{optimization}) and~(\ref{optimization2}) give the same numerical result provides a cross-check of the correctness of the procedure.

Now, if rather than eq. (\ref{LagrangeFormula}), one uses a logarithmic interpolation of the form:
\begin{equation}\label{LagrangeFormulaLog}
w_{\alpha}^{(k)}(x) = \sum_{j=0,j \leq \alpha}^{k}\theta(x-x_{\alpha-j})\theta(x_{\alpha-j+1}-x)\prod^{k}_{\delta=0,\delta\ne j}\left[\frac{\ln(x)-\ln(x_{\alpha-j+\delta})}{\ln(x_{\alpha})-\ln(x_{\alpha-j+\delta})}\right]
\end{equation}
over a logarithmically distributed grid, i.e. such that $\ln(x_{\beta})-\ln(x_{\alpha})=(\beta-\alpha)\delta x$, where the step $\delta x$ is a constant, one has that:
\begin{equation}\label{LagrangeFormulaLog}
w_{\alpha}^{(k)}(x) = \sum_{j=0,j \leq \alpha}^{k}\theta(x-x_{\alpha-j})\theta(x_{\alpha-j+1}-x)\prod^{k}_{\delta=0,\delta\ne j}\left[\frac{1}{\delta x} \ln\left(\frac{x}{x_\alpha}\right)\frac{1}{j-\delta}+1\right]
\end{equation}
that in general means that $w_\alpha^{(k)}(x)\equiv w_\alpha^{(k)}[\ln(x)-\ln(x_\alpha)]$. Therefore in eq.~(\ref{optimization2}) we have that:
\begin{equation}
\begin{array}{rcl}
\displaystyle w_{\alpha}^{(k)}\left(\frac{x_\beta}{y}\right) &\equiv& w_\alpha^{(k)}[\ln(x_{\beta})-\ln(x_\alpha)-\ln(y)]\\
\\
                                      &=&w_\alpha^{(k)}[(\beta-\alpha)\delta x-\ln(y)]
\end{array}
\end{equation}
which means that $w_{\alpha}^{(k)}(x_\beta/y)$ only depends on the difference $(\beta-\alpha)$ with the consequence that also $\Pi_{ij,\beta\alpha}$ only depends on $(\beta-\alpha)$. Now, one can use this information with eq.~(\ref{nonzero2}) to represent $\Pi_{ij,\beta\alpha}(t)$ as a matrix, where $\beta$ is the row index and $\alpha$ the column index. In this way  $\Pi_{ij,\beta\alpha}(t)$ would look like this:
\begin{equation}\label{MatrixRep}
\displaystyle \Pi_{ij}(t) = 
\begin{pmatrix}
a_0 &  a_1 & a_2 & \cdots & a_{N_x} \\
 0  & a_0 & a_1 & \cdots & a_{N_x-1} \\
 0  & 0   &  a_0 & \cdots & a_{N_x-2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
 0  &   0  &   0 & \cdots & a_0 
\end{pmatrix}
\end{equation}
therefore, if one knows the first raw of the matrix above, i.e. $\Pi_{ij,0\alpha}(t)$, it is possible to reconstruct the whole matrix. Of course, this feature must be numerically verified but it makes the computation of the evolution operators much faster because it reduces the number of integrals to be computed by a factor $N_x$.


\section{Splitting Functions Treatment}

The general form DGLAP splitting functions is the following:
\begin{equation}
\tilde{P}_{ij}(x,t) = xP_{ij}^{R}(x,t) + \frac{xP_{ij}^{S}(t)}{(1-x)_+} + P_{ij}^{L}(t)x\delta(1-x)
\end{equation}
where $P_{ij}^{R}(x,t)$ is the regular term that can be integrated without any problem over any range, $P_{ij}^{S}(x,t)$ is instead the function that multiplies the singular term which is regularized by means of the plus prescription whose definition, referring to eq.~(\ref{optimization2}), is:
\begin{equation}
\begin{array}{c}
\displaystyle \int_c^d dy \frac{f(y)}{(1-y)_+} = \int_c^d dy \frac{f(y) - f(1)\theta(d-1)}{1-y} - f(1)\theta(d-1)\int_0^c \frac{dy}{1-y} = \\
\\
\displaystyle \int_c^d dy \frac{f(y) - f(1)\theta(d-1)}{1-y} + f(1)\ln(1-c)\theta(d-1)\,.
\end{array}
\end{equation}
Finally $P_{ij}^{L}(t)$ is the coefficient of the local term, i.e. the term proportional to $\delta(1-x)$. Each of these terms has a perturbative expansion that at N$^k$LO looks like this:
\begin{equation}
P_{ij}^{J}(x,t) = \sum_{n=0}^{k}a_s^{n+1}(t)P_{ij}^{J,(n)}(x)\qquad\mbox{with}\qquad J=R,S,L
\end{equation}
Therefore one has that:
\begin{equation}
\begin{array}{c}
\displaystyle \Pi_{ij,\beta\alpha}(t) = \\
\\
\displaystyle \sum_{n=0}^{k} a_s^{n+1}(t) \bigg\{\int^{d}_{c}dy\left[{P}_{ij}^{R,(n)}(y)w_{\alpha}\left(\frac{x_\beta}{y}\right)+\frac{{P}_{ij}^{S,(n)}}{1-y}\left(w_{\alpha}\left(\frac{x_\beta}{y}\right)-w_{\alpha}^{(k)}(x_\beta)\theta(d-1)\right)\right]\\
\\
\displaystyle +\left[{P}_{ij}^{S,(n)}\ln(1-c)\theta(d-1)+{P}_{ij}^{L,(n)}\right]w_{\alpha}^{(k)}(x_\beta)\bigg\}\,.
\end{array}
\end{equation}
Moreover it is easy to see that $w_{\alpha}^{(k)}(x_\beta)=\delta_{\beta\alpha}$, so that:
\begin{equation}
\begin{array}{c}
\displaystyle \Pi_{ij,\beta\alpha}(t) = \\
\\
\displaystyle \sum_{n=0}^{k} a_s^{n+1}(t) \bigg\{\int^{d}_{c}dy\left[{P}_{ij}^{R,(n)}(y)w_{\alpha}\left(\frac{x_\beta}{y}\right)+\frac{{P}_{ij}^{S,(n)}}{1-y}\left(w_{\alpha}\left(\frac{x_\beta}{y}\right)-\delta_{\beta\alpha}\theta(d-1)\right)\right]\\
\\
\displaystyle +\left[{P}_{ij}^{S,(n)}\ln(1-c)\theta(d-1)+{P}_{ij}^{L,(n)}\right]\delta_{\beta\alpha}\bigg\}\,.
\end{array}
\end{equation}

Calling:
\begin{equation}\label{pertExp}
\begin{array}{c}
\displaystyle \Pi_{ij,\beta\alpha}^{(n)} = \\
\\
\displaystyle \int^{d}_{c}dy\left[{P}_{ij}^{R,(n)}(y)w_{\alpha}\left(\frac{x_\beta}{y}\right)+\frac{{P}_{ij}^{S,(n)}}{1-y}\left(w_{\alpha}\left(\frac{x_\beta}{y}\right)-\delta_{\beta\alpha}\theta(d-1)\right)\right]\\
\\
\displaystyle +\left[{P}_{ij}^{S,(n)}\ln(1-c)\theta(d-1)+{P}_{ij}^{L,(n)}\right]\delta_{\beta\alpha}\,,
\end{array}
\end{equation}
we have that:
\begin{equation}\label{splittingexp}
\Pi_{ij,\beta\alpha}(t) = \sum_{n=0}^{k} a_s^{n+1}(t) \Pi_{ij,\beta\alpha}^{(n)}\,,
\end{equation}
and the integrals $\Pi_{ij,\beta\alpha}^{(n)}$ do not depend on the energy therefore, once the grid (and the number of active flavours) has been fixed, they can be evaluate once and for all at the beginning and used for the evolution to any scale.

It is not very easy to see that eq.~(\ref{pertExp}) respects the symmetry described in eq.~(\ref{MatrixRep}). To show this, we distinguish two case: 1) $d < 1$ and 2) $d = 1$. In the case 1), due to the presence of the Heaviside's functions $\theta(d-1)$, eq.~(\ref{pertExp}) reduces to:
\begin{equation}\label{pertExp2}
\Pi_{ij,\beta\alpha}^{(n)} = \int^{d}_{c}dy\left[{P}_{ij}^{R,(n)}(y)+\frac{{P}_{ij}^{S,(n)}}{1-y}\right]w_{\alpha}\left(\frac{x_\beta}{y}\right) + {P}_{ij}^{L,(n)}\delta_{\beta\alpha}\,,
\end{equation}
which evidently obeys eq.~(\ref{MatrixRep}). In the case 2), instead, we have:
\begin{equation}\label{pertExp3}
\begin{array}{c}
\displaystyle \Pi_{ij,\beta\alpha}^{(n)} = \int^{1}_{c}dy\left[{P}_{ij}^{R,(n)}(y)w_{\alpha}\left(\frac{x_\beta}{y}\right)+\frac{{P}_{ij}^{S,(n)}}{1-y}\left(w_{\alpha}\left(\frac{x_\beta}{y}\right)-\delta_{\beta\alpha}\right)\right]\\
\\
\displaystyle +\left[{P}_{ij}^{S,(n)}\ln(1-c)+{P}_{ij}^{L,(n)}\right]\delta_{\beta\alpha}\,,
\end{array}
\end{equation}
and apparently, if $\alpha=\beta$, the term $\ln(1-c)$ seems to break the symmetry. However, this is not the case. In fact, from eq.~(\ref{bounds2}), we know that in this particular case:
\begin{equation}
c = \mbox{max}(x_\beta,x_\beta/x_{\beta+1}) = \frac{x_\beta}{x_{\beta+1}}
\end{equation}
because $x_{\beta+1}<1$. In addition, on a logarithmically distributed grid, $x_{\beta+1}=x_{\beta}\exp(\delta x)$, where $\delta x$ is the constant step. Therefore, it turns out that:
\begin{equation}
\ln(1-c) = \ln\left(1-\frac{x_{\beta}}{x_{\beta+1}}\right) = \ln[1 - \exp(-\delta x)]\,,
\end{equation}
that is a constant which does not depend on the indices $\alpha$ and $\beta$ and therefore does not break the symmetry given in eq.~(\ref{MatrixRep}).


\section{Solution of the DGLAP Equation}

As a consequence of the DGLAP equation form, we can assume that $\tilde{q}_{i}(x_\beta,t)\equiv q_{i,\beta}(t)$ evolves between the energies $t$ and $t_0$ according to the following (discretized) evolution equation:
\begin{equation}
\tilde{q}_{i,\beta}(t) = \sum_{\gamma,k} M_{ik,\beta\gamma}(t,t_0)\tilde{q}_{k,\gamma}(t_0) 
\end{equation}
with the boundary condition $M_{ik,\beta\gamma}(t_0,t_0)=\delta_{ik}\delta_{\beta\gamma}$. It follows that eq. (\ref{dglap4}) takes the form:
\begin{equation}\label{tosolve}
\left\{\begin{array}{l}
\displaystyle \frac{\partial  M_{ij,\alpha\beta}(t,t_0)}{\partial t}=\sum_{\gamma,k} \Pi_{ik,\alpha\gamma}(t)M_{kj,\gamma\beta}(t,t_0)\\
\\
\displaystyle M_{ij,\alpha\beta}(t_0,t_0)=\delta_{ij}\delta_{\alpha\beta}
\end{array}\right.
\end{equation}
which is a first order linear differential equation in the quantity
$M_{ij,\alpha\beta}(t,t_0)$ that, as we actually do, can be
numerically solved using the fourth order Adaptive Step-size Control Runge-Kutta algorithm.
Using the arguments we discussed above, we do not need to compute all
the entries of $\Pi_{ik,\gamma\alpha}(t)$. In addition, as we have already shown, the
perturbative contributions to $\Pi_{ik,\gamma\alpha}(t)$ can be
precomputed before solving the differential equation in eq. (\ref{tosolve}).


\subsection{The Non Singlet}

The non-singlet case is the easiest one because the differential equations in eq.~(\ref{tosolve}) decouple in the flavour pair $(i,j)$, and we can write them as:
\begin{equation}\label{tosolveNS}
\left\{\begin{array}{l}
\displaystyle \frac{\partial  \mathcal{M}_{\alpha\beta}^{(i)}(t,t_0)}{\partial t}=\sum_{\gamma=0}^{N_x} \mathcal{P}_{\alpha\gamma}^{(i)}(t)\mathcal{M}_{\gamma\beta}^{(i)}(t,t_0)\\
\\
\displaystyle \mathcal{M}_{\alpha\beta}^{(i)}(t_0,t_0)=\delta_{\alpha\beta}
\end{array}\right.\quad\mbox{with } i=+,-,V\,.
\end{equation}
As a further simplification, we can use the fact that at LO $+$, $-$ and $V$ all the evolution operators are equal, therefore solving only one of them is enough. at NLO instead only $-$ and $V$ are equal while at NNLO they are all different.

Now, given the symmetries carried by $\Pi_{ij,\alpha\beta}$, we can write:
\begin{equation}
\mathcal{P}_{\alpha\gamma}^{(i)} = \mathcal{P}_{0(\gamma-\alpha)}^{(i)}\theta(\gamma-\alpha)\,,
\end{equation}
so that eq.~(\ref{tosolveNS}) becomes:
\begin{equation}\label{tosolveNS1}
\left\{\begin{array}{l}
\displaystyle \frac{\partial  \mathcal{M}_{\alpha\beta}^{(i)}(t,t_0)}{\partial t}= \sum_{\delta=0}^{N_x-\alpha} \mathcal{P}_{0\delta}^{(i)}(t)\mathcal{M}_{(\alpha+\delta)\beta}^{(i)}(t,t_0)\\
\\
\displaystyle \mathcal{M}_{\alpha\beta}^{(i)}(t_0,t_0)=\delta_{\alpha\beta}
\end{array}\right.\quad\mbox{with } i=+,-,V\,.
\end{equation}

\subsection{The Singlet}

The singlet sector is totally analogous to the non-singlet one, the only difference is that there is one additional summation over the flavours. In practice we have that:
\begin{equation}\label{tosolveSG}
\left\{\begin{array}{l}
\displaystyle \frac{\partial  \mathcal{M}_{ij,\alpha\beta}^{\rm SG}(t,t_0)}{\partial t}= \sum_k \sum_{\delta=0}^{N_x-\alpha} \mathcal{P}_{ik,0\delta}^{\rm SG}(t)\mathcal{M}_{kj,(\alpha+\delta)\beta}^{\rm SG}(t,t_0)\\
\\
\displaystyle \mathcal{M}_{ij,\alpha\beta}^{\rm SG}(t_0,t_0)=\delta_{ij}\delta_{\alpha\beta}
\end{array}\right.\quad\mbox{with }i,j,k=q,g\,.
\end{equation}

\section{Alternative Solutions of the DGLAP Equation}

In this section we show how it is possible to solve the DGLAP equation
in an alternative way with respect to that shown in the previous
sections exploiting the RGE of the running coupling $\alpha_s$. This
will lead to a different equation that admits two preturbatively
equivalent solutions: the first, that we will refer to as "exact"
solution and that reproduces the solution seen above, and the second, the
so-called "expanded" solution, that reproduces the solution usually
adopted in the N-space code (like NNPDF). 

The starting point is the RGE:
\begin{equation}\label{RGEalpha}
\mu^2\frac{\partial a_s}{\partial \mu^2} = \frac{\partial
  a_s}{\partial t} = \beta(a_s)\,,
\end{equation}
where:
\begin{equation}
a_s \equiv \frac{\alpha_s}{4\pi}
\end{equation}
and:
\begin{equation}
\beta(a_s) = -a_s^2 \sum_{n=0}^{N} a_s^n \beta_n \,.
\end{equation}
where $N$ represents the desired preturbative order.
Using eq. (\ref{RGEalpha}) and eq. (\ref{splittingexp}), we can
rewrite eq. (\ref{tosolve}) as:
\begin{equation}\label{tosolvealpha}
\left\{\begin{array}{l}
\displaystyle \frac{\partial  M_{ij,\alpha\beta}(t,t_0)}{\partial
  a_s}= -\frac{1}{a_s}\sum_{\gamma,k} \left[\frac{\displaystyle \sum_{n=0}^N a_s^n
    \Pi_{ik,\alpha\gamma}^{(n)}}{ \displaystyle \sum_{n=0}^{N} a_s^n \beta_n}\right]M_{kj,\gamma\beta}(t,t_0)\\
\\
\displaystyle M_{ij,\alpha\beta}(t_0,t_0)=\delta_{ij}\delta_{\alpha\beta}
\end{array}\right.
\end{equation}

Now there are two possibile way to solve eq. (\ref{tosolvealpha}):
either we solve directly it numerically as it is or we first expand
the term in the square brackets as a series of $a_s$ keeping only the
terms up to order $a_s^N$ and then we solve the equation. It is
obvious that the first way to solve eq. (\ref{tosolvealpha}) must be
numerical equal to the solution of
eq. (\ref{tosolve}). The second way instead is not numerically equal
but is perturbatively equivalent. This second solution is referred to
as N-space solution as it is usually used in the N-space approach
because it permits to solve analytically the DGLAP equation.

To expand the term in the square brackets, we notice that up to NNLO ($N=2$)
we have that:
\begin{equation}
\frac{1}{\displaystyle \sum_{n=0}^{2} a_s^n \beta_n} =
\frac1{\beta_0}\left[ 1 - \frac{\beta_1}{\beta_0} a_s + \left(\frac{\beta_1^2}{\beta_0^2}- \frac{\beta_2}{\beta_0}\right)a_s^2\right] + \mathcal{O}(a_s^{3})\,,
\end{equation}
so that:
\begin{equation}
\begin{array}{rcl}
\displaystyle\frac{\displaystyle \sum_{n=0}^2 a_s^n
    \Pi_{ik,\alpha\gamma}^{(n)}}{ \displaystyle \sum_{n=0}^{2} a_s^n
    \beta_n} &=&\displaystyle \frac{1}{\beta_0}\left\{\Pi_{ik,\alpha\gamma}^{(0)} +
a_s\left[\Pi_{ik,\alpha\gamma}^{(1)} - b_1 \Pi_{ik,\alpha\gamma}^{(0)}\right]\right.\\
\\
&+& \displaystyle
\left. a_s^2\left[\Pi_{ik,\alpha\gamma}^{(2)} - b_1
\Pi_{ik,\alpha\gamma}^{(1)} +\left(b_1^2-b_2\right)
\Pi_{ik,\alpha\gamma}^{(0)} \right]\right\}  + \mathcal{O}(a_s^{3})
\end{array}
\end{equation}
where we have defined:
\begin{equation}
b_n \equiv \frac{\beta_n}{\beta_0}\,.
\end{equation}
Finally, defining:
\begin{equation}
\begin{array}{l}
\displaystyle\widetilde{\Pi}_{ik,\alpha\gamma}^{(0)} \equiv
\Pi_{ik,\alpha\gamma}^{(0)} \,,\\
\displaystyle\widetilde{\Pi}_{ik,\alpha\gamma}^{(1)} \equiv
\Pi_{ik,\alpha\gamma}^{(1)} - b_1 \Pi_{ik,\alpha\gamma}^{(0)} \,,\\
\displaystyle\widetilde{\Pi}_{ik,\alpha\gamma}^{(2)} \equiv \Pi_{ik,\alpha\gamma}^{(2)} - b_1
\Pi_{ik,\alpha\gamma}^{(1)} +\left(b_1^2-b_2\right)
\Pi_{ik,\alpha\gamma}^{(0)}\,,
\end{array}
\end{equation}
we can write eq. (\ref{tosolvealpha}) up to NNLO as:
\begin{equation}\label{tosolvealphatrn}
\left\{\begin{array}{l}
\displaystyle \frac{\partial  M_{ij,\alpha\beta}(t,t_0)}{\partial
  a_s}= -\frac{1}{a_s\beta_0}\sum_{\gamma,k} \left[\displaystyle \sum_{n=0}^2 a_s^n
    \widetilde{\Pi}_{ik,\alpha\gamma}^{(n)}\right]M_{kj,\gamma\beta}(t,t_0)\\
\\
\displaystyle M_{ij,\alpha\beta}(t_0,t_0)=\delta_{ij}\delta_{\alpha\beta}
\end{array}\right.\,.
\end{equation}
Solving eq. (\ref{tosolvealphatrn}) provides the so-called "expanded" solution.


\end{document}